{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI ëª¨ë¸ í™œìš© 1ì£¼ì°¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3_envs\\SpartaCodingClub\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "12.4\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My name is\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[3666, 1438,  318]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(encoded_input['input_ids'], max_length=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My name is John. I'm a man of God. I'm a man of God. I'm a man of God. I'm a man of God. I'm a man of God. I'm a man of God. I'm a\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3666, 1438,  318, 1757,   13,  314, 1101,  257,  582,  286, 1793,   13,\n",
       "          314, 1101,  257,  582,  286, 1793,   13,  314, 1101,  257,  582,  286,\n",
       "         1793,   13,  314, 1101,  257,  582,  286, 1793,   13,  314, 1101,  257,\n",
       "          582,  286, 1793,   13,  314, 1101,  257,  582,  286, 1793,   13,  314,\n",
       "         1101,  257]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI ëª¨ë¸ í™œìš© 2ì£¼ì°¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorchë¥¼ í™œìš©í•˜ì—¬ Transformer ëª¨ë¸ êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3_envs\\SpartaCodingClub\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ ì¤€ë¹„\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor epoch in range(num_epochs):\\n    optimizer.zero_grad()\\n    output = model(src, tgt)\\n    loss = criterion(output, tgt_labels)\\n    loss.backward()\\n    optimizer.step()\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ\n",
    "\"\"\"\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, tgt)\n",
    "    loss = criterion(output, tgt_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ í™œìš©í•˜ê¸°\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "cache_path = 'D:\\Anaconda3_envs\\SpartaCodingClub\\SpartaCodingClub\\main_camp'\n",
    "\n",
    "torch.hub.set_dir(cache_path)\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2', cache_dir=cache_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', cache_dir=cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'Once upon a time'\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids, max_length=50, num_return_sequences = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI ëª¨ë¸ í™œìš© 3ì£¼ì°¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert_Basic.py & transformers_code.py ì°¸ê³ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI ëª¨ë¸ í™œìš© 4ì£¼ì°¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time the same happened to the ancient Egyptian, the Egyptian man who had long ago vanquished the evil gods is made a mortal for each of a man's deeds. Now it is clear that the same god is given immortality to a man that\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# GPT-2 ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ë¡œë“œ\n",
    "generator = pipeline(\"text-generation\", model='gpt2', device=device)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìƒì„±\n",
    "generated_text = generator(\"Once upon a time\", max_length=50, num_return_sequences=1, truncation=True)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: ì•ˆë…•! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ! í˜¹ì‹œ ì—¬ê¸° ì˜¬ ë•Œ \"ì „ììš°ì²´êµ­\"ì—ì„œ ë¹„í–‰ê¸°ë¡œ ì˜¤ì…¨ë‚˜ìš”? ì•„ë‹ˆë©´ ë¹›ì˜ ì†ë„ë¡œ ì›Œí”„í•˜ì…¨ë‚˜ìš”?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"ë„ˆëŠ” í™˜ì˜ ì¸ì‚¬ë¥¼ í•˜ëŠ” ì¸ê³µì§€ëŠ¥ì´ì•¼, ë†ë‹´ì„ ë„£ì–´ ì¬ë¯¸ìˆê²Œí•´ì¤˜\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•ˆë…•?\"}  \n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].message.content)\n",
    "\n",
    "# ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì €ë‘ ì–˜ê¸°í•˜ë‹¤ê°€ ì¬ë¯¸ ì—†ìœ¼ë©´ ì´ë ‡ê²Œ ìƒê°í•´ë³´ì„¸ìš”: ì ì–´ë„ ì—‰ë©ì´ì— ê¼¬ë¦¬ ë‹¬ë¦° ì›ìˆ­ì´ì™€ëŠ” ë‹¤ë¥´ê²Œ, ì €ëŠ” í‰ë²”í•˜ê²Œ ë¬´ë¦¬í•˜ì§€ ì•Šê±°ë“ ìš”! ë­ë“  ë¬¼ì–´ë³´ì„¸ìš”, ë„ì™€ë“œë¦´ê²Œìš”! ğŸ˜„ \n",
    "\n",
    "# ê°•ì‚¬ì˜ í•œë§ˆë””: ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:53<00:00,  7.58s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:09<00:00,  5.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to('cuda')  # GPU ì‚¬ìš©\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±\n",
    "prompt = \"A futuristic cityscape with flying cars at sunset\"\n",
    "image = pipe(prompt).images[0]\n",
    "\n",
    "# ìƒì„±ëœ ì´ë¯¸ì§€ ì €ì¥ ë° ì¶œë ¥\n",
    "image.save(\"generated_image.png\")\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI ëª¨ë¸ í™œìš© 5ì£¼ì°¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API ì‹¬í™” ë° ê°„ë‹¨í•œ ì‹¤ìŠµ (FastAPI í™œìš©) -> FastAPI.py ì°¸ê³ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPTì™€ ElevenLabs ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: ì•ˆë…•í•˜ì„¸ìš”, ìœ¤ìˆ˜ìš©ë‹˜. ë²•ë¥  ìƒë‹´ì´ í•„ìš”í•˜ì‹ ê°€ìš”? ì–´ë–¤ ì ì´ ê¶ê¸ˆí•˜ì‹ ì§€ ë§ì”€í•´ì£¼ì‹œë©´ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ë°©í–¥ìœ¼ë¡œ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# API í‚¤ë¥¼ ëª» ê°€ì ¸ì™”ì„ ê²½ìš° ì—ëŸ¬ í‘œì‹œ \n",
    "if api_key is None:\n",
    "    raise ValueError('Failed loading')\n",
    "\n",
    "client = OpenAI()\n",
    "client.api_key = api_key\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"ë„ˆëŠ” ë³€í˜¸ì‚¬ì•¼ ë‚˜ì—ê²Œ ë²•ì ì¸ ìƒë‹´ì„ í•´ì¤˜\"},\n",
    "        {\"role\": \"user\", \"content\": \"ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” ìœ¤ìˆ˜ìš©ì…ë‹ˆë‹¤.\"}  \n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€ë‹µ:  ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë²•ë¥  ê´€ë ¨ ìƒë‹´ì´ í•„ìš”í•˜ì‹ ê°€ìš”?\n",
      "ëŒ€ë‹µ:  ë¶€ê°€ê°€ì¹˜ì„¸(VAT)ëŠ” ìƒí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ê°€ ìƒì‚° ë° ìœ í†µë˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë¶€ê°€ê°€ì¹˜ì— ëŒ€í•´ ë¶€ê³¼ë˜ëŠ” ì„¸ê¸ˆì„ ë§í•©ë‹ˆë‹¤. í•œêµ­ì—ì„œëŠ” ì‚¬ì—…ìê°€ ì†Œë¹„ìë¡œë¶€í„° ìƒí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ íŒë§¤ ì‹œ ë¶€ê°€ê°€ì¹˜ì„¸ë¥¼ ì§•ìˆ˜í•˜ê³ , ì´í›„ ì •ë¶€ì— ì´ë¥¼ ë‚©ë¶€í•©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œëŠ” íŒë§¤ ê°€ê²©ì— 10%ì˜ ì„¸ìœ¨ì´ ì ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "ë¶€ê°€ê°€ì¹˜ì„¸ì˜ íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” ì‚¬ì—…ìê°€ ë§¤ì… ì‹œ ë¶€ë‹´í–ˆë˜ ë¶€ê°€ê°€ì¹˜ì„¸ë¥¼ ê³µì œë°›ì„ ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì¦‰, ì‚¬ì—…ìëŠ” íŒë§¤ ì‹œ ì§•ìˆ˜í•œ ë¶€ê°€ê°€ì¹˜ì„¸ì—ì„œ ë§¤ì… ì‹œ ë‚©ë¶€í•œ ë¶€ê°€ê°€ì¹˜ì„¸ë¥¼ ì°¨ê°í•œ ê¸ˆì•¡ë§Œí¼ì„ êµ­ê°€ì— ë‚©ë¶€í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì´ì¤‘ê³¼ì„¸ë¥¼ ë°©ì§€í•˜ê³ , ì„¸ê¸ˆ ë¶€ë‹´ì´ ìµœì¢… ì†Œë¹„ìì—ê²Œ ì „ê°€ë˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë¶€ê°€ê°€ì¹˜ì„¸ëŠ” ëŒ€ë¶€ë¶„ì˜ ìƒí’ˆê³¼ ì„œë¹„ìŠ¤ì— ì ìš©ë˜ì§€ë§Œ, ì¼ë¶€ ë©´ì„¸ ëŒ€ìƒë„ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê¸°ì´ˆ ì‹ë£Œí’ˆ, ì˜ë£Œ ì„œë¹„ìŠ¤, êµìœ¡ ì„œë¹„ìŠ¤ ë“±ì€ ë¶€ê°€ê°€ì¹˜ì„¸ê°€ ë©´ì œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ì—…ìëŠ” ì •ê¸°ì ìœ¼ë¡œ ë¶€ê°€ê°€ì¹˜ì„¸ ì‹ ê³ ì™€ ë‚©ë¶€ë¥¼ í•´ì•¼ í•˜ë©°, ì´ë¥¼ í†µí•´ êµ­ê°€ì˜ ì„¸ìˆ˜ í™•ë³´ì™€ ê²½ì œ í™œë™ì˜ íˆ¬ëª…ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.\n",
      "ëŒ€ë‹µ:  ì¦ê±°ìš´ ëŒ€í™”ì˜€ìŠµë‹ˆë‹¤! ê°ì‚¬í•©ë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "system_message =  {\n",
    "    \"role\": \"system\", \"content\": \"ë„ˆëŠ” ë³€í˜¸ì‚¬ì•¼ ë‚˜ì—ê²Œ ë²•ë¥ ì ì¸ ìƒë‹´ì„ í•´ì¤˜. ê·¸ë¦¬ê³  ì£¼ì˜ì‚¬í•­ì€ ë§í•˜ì§€ë§ˆ. í•œêµ­ë²•ì„ ê¸°ì¤€ìœ¼ë¡œ \"\n",
    "}\n",
    "\n",
    "messages = [system_message]\n",
    "\n",
    "while True :\n",
    "    user_input = input(\"ì‚¬ìš©ì ì „ë‹¬:\")\n",
    "    if user_input == \"exit\":\n",
    "        print(\"ëŒ€ë‹µ:  ì¦ê±°ìš´ ëŒ€í™”ì˜€ìŠµë‹ˆë‹¤! ê°ì‚¬í•©ë‹ˆë‹¤!\")\n",
    "        break\n",
    "\n",
    "    messages.append({\"role\" : \"user\" , \"content\" : user_input })\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages = messages\n",
    "    )\n",
    "\n",
    "    reply = completion.choices[0].message.content\n",
    "    print(\"ëŒ€ë‹µ:  \" + reply)\n",
    "    messages.append({\"role\" : \"assistant\" , \"content\" : reply })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    Error 263 for command:\n",
      "        open output_audio.mp3\n",
      "    ì§€ì •í•œ ì¥ì¹˜ê°€ ì—´ë ¤ ìˆì§€ ì•Šê±°ë‚˜ MCIì—ì„œ ì¸ì‹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "\n",
      "    Error 263 for command:\n",
      "        close output_audio.mp3\n",
      "    ì§€ì •í•œ ì¥ì¹˜ê°€ ì—´ë ¤ ìˆì§€ ì•Šê±°ë‚˜ MCIì—ì„œ ì¸ì‹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "Failed to close the file: output_audio.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Wrote audio to output_audio.mp3\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_25420\\379441704.py\", line 53, in <module>\n",
      "    playsound.playsound(output_filename)\n",
      "  File \"d:\\Anaconda3_envs\\SpartaCodingClub\\lib\\site-packages\\playsound.py\", line 72, in _playsoundWin\n",
      "  File \"d:\\Anaconda3_envs\\SpartaCodingClub\\lib\\site-packages\\playsound.py\", line 64, in winCommand\n",
      "    url   = NSURL.URLWithString_(sound)\n",
      "playsound.PlaysoundException: \n",
      "    Error 263 for command:\n",
      "        open output_audio.mp3\n",
      "    ì§€ì •í•œ ì¥ì¹˜ê°€ ì—´ë ¤ ìˆì§€ ì•Šê±°ë‚˜ MCIì—ì„œ ì¸ì‹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\", line 812, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\ultratb.py\", line 730, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\executing\\executing.py\", line 168, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import requests\n",
    "import playsound\n",
    "from pydub import AudioSegment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ API í‚¤ì™€ URL ê°€ì ¸ì˜¤ê¸°\n",
    "api_key = os.getenv(\"ELEVENLABS_API_KEY\")\n",
    "url =os.getenv(\"ELEVENLABS_API_URL\")\n",
    "\n",
    "# API í‚¤ or URL ì„ ëª» ê°€ì ¸ì™”ì„ ê²½ìš° ì—ëŸ¬ í‘œì‹œ \n",
    "if api_key is None or url is None:\n",
    "    raise ValueError('Failed loading')\n",
    "\n",
    "output_filename = \"output_audio.mp3\"\n",
    "\n",
    "headers = {\n",
    "    \"xi-api-key\": api_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "text = input(\"ìƒì„±í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# ìŒì„± ìƒì„± ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.\n",
    "data = {\n",
    "    \"text\": text,\n",
    "    \"model_id\": \"eleven_multilingual_v2\",\n",
    "    \"voice_settings\": {\n",
    "        \"stability\": 0.3,\n",
    "        \"similarity_boost\": 1,\n",
    "        \"style\": 1,\n",
    "        \"use_speaker_boost\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers, stream=True)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    audio_content = b\"\"\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            audio_content += chunk\n",
    "\n",
    "    segment = AudioSegment.from_mp3(io.BytesIO(audio_content))\n",
    "    segment.export(output_filename, format=\"mp3\")\n",
    "    print(f\"Success! Wrote audio to {output_filename}\")\n",
    "\n",
    "    # ì˜¤ë””ì˜¤ë¥¼ ì¬ìƒí•©ë‹ˆë‹¤.\n",
    "    playsound.playsound(output_filename)\n",
    "else:\n",
    "    print(f\"Failed to save file: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultralytics YOLOë¥¼ í™œìš©í•œ ì´ë¯¸ì§€ ê°ì²´ íƒì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\USER\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpartaCodingClub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
