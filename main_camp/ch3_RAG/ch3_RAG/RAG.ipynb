{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM & RAG를 활용한 AI 서비스 만들기 - 5주차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Vector DB 개념 및 RAG (Retrieval-Augmented Generation) 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 데이터 임베딩 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3_envs\\Sparta\\main_camp\\ch3_RAG\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "d:\\Anaconda3_envs\\Sparta\\main_camp\\ch3_RAG\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1024)\n",
      "[ 0.03305342  0.00946585 -0.00015371 ... -0.02273227  0.00190389\n",
      "  0.01841401]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Multilingual-E5-large-instruct 모델 로드\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large').to(device)\n",
    "\n",
    "# 문장 리스트\n",
    "sentences = [\n",
    "    \"참새는 짹짹하고 웁니다.\",\n",
    "    \"LangChain과 Faiss를 활용한 예시입니다.\",\n",
    "    \"자연어 처리를 위한 임베딩 모델 사용법을 배워봅시다.\",\n",
    "    \"유사한 문장을 검색하는 방법을 살펴보겠습니다.\",\n",
    "    \"강좌를 수강하시는 수강생 여러분 감사합니다!\"\n",
    "]\n",
    "\n",
    "# 문장들을 임베딩으로 변환\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# 임베딩 벡터 출력\n",
    "print(embeddings.shape)  # 4개의 문장이 1024 차원의 벡터로 변환됨\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Python LangChain과 FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# .env 파일에서 OPENAI API KEY 가져오기\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain 기본 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 언어 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자의 요청에 따라 다양한 정보 제공, 일정 관리, 알람 설정, 뉴스 업데이트, 음악 재생 등을 도와드릴 수 있습니다. 구체적으로 어떤 도움이 필요한지 알려주시면 감사하겠습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# 모델에 메시지 전달\n",
    "response = model.invoke([HumanMessage(content=\"안녕하세요, 무엇을 도와드릴까요?\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 프롬프트 템플릿 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='Translate the following sentence from English to French:', additional_kwargs={}, response_metadata={}), HumanMessage(content='How are you?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 시스템 메시지 설정\n",
    "system_template = \"Translate the following sentence from English to {language}:\"\n",
    "\n",
    "# 사용자 텍스트 입력\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "\n",
    "# 프롬프트 생성\n",
    "result = prompt_template.invoke({\"language\": \"French\", \"text\": \"How are you?\"})\n",
    "print(result.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Expression Language (LCEL)로 체인 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "도서관은 어디에 있나요?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 응답을 파싱하는 파서 초기화\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 템플릿, 모델, 파서를 체인으로 연결\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "# 체인 실행\n",
    "response = chain.invoke({\"language\": \"Korean\", \"text\": \"Where is the library?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAISS를 활용한 벡터 데이터베이스 구성 및 쿼리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: OpenAI 임베딩 모델로 벡터 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: FAISS 인덱스 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: 벡터 데이터베이스에 문서 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4068b581-3ca8-4d84-8327-05481b4ae96f',\n",
       " 'e3973e77-b0f7-42b1-8a1a-8320929475ae',\n",
       " 'd8d7cc6c-0975-46bd-af7a-d62e59ef4f1c',\n",
       " 'bc5e69d5-355b-4b00-a532-c15ea053d6c1']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from uuid import uuid4  # 고유 ID를 생성해주는 라이브러리\n",
    "\n",
    "# 문서 생성\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain을 사용해 프로젝트를 구축하고 있습니다!\", metadata={\"source\":\"tweet\"}),\n",
    "    Document(page_content=\"내일 날씨는 맑고 따뜻할 예정입니다.\", metadata={\"source\":\"news\"}),\n",
    "    Document(page_content=\"오늘 아침에는 팬케이크와 계란을 먹었어요.\", metadata={\"source\":\"personal\"}),\n",
    "    Document(page_content=\"주식 시장이 경기 침체 우려로 하락 중입니다.\", metadata={\"source\":\"news\"}),\n",
    "]\n",
    "\n",
    "# 고유 ID 생성 및 문서 추가\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: 벡터 데이터베이스 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 내일 날씨는 맑고 따뜻할 예정입니다. [{'source': 'news'}]\n",
      "* 주식 시장이 경기 침체 우려로 하락 중입니다. [{'source': 'news'}]\n",
      "* [SIM=0.159] LangChain을 사용해 프로젝트를 구축하고 있습니다! [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "# 기본 유사성 검색\n",
    "results = vector_store.similarity_search(\"내일 날씨는 어떨까요?\", k=2, filter={\"source\": \"news\"})\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "    \n",
    "# 점수와 함께 유사성 검색\n",
    "results_with_scores = vector_store.similarity_search_with_score(\"LangChain에 대해 이야기해주세요.\", k=2, filter={\"source\": \"tweet\"})\n",
    "for res, score in results_with_scores:\n",
    "    print(f\"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG 체인에 FAISS 통합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Retriever로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: RAG 체인 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug Output: 강사 이름은?\n",
      "Debug Output: {'context': [Document(metadata={'source': 'tweet'}, page_content='LangChain을 사용해 프로젝트를 구축하고 있습니다!')], 'question': '강사 이름은?'}\n",
      "Final Response:\n",
      "The context does not provide information on the instructor's name.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "    \n",
    "# 문서 리스트를 텍스트로 변환하는 단계 추가\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):  # config 인수 추가\n",
    "        # context의 각 문서를 문자열로 결합\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "    \n",
    "# RAG 체인에서 각 단계마다 DebugPassThrough 추가\n",
    "rag_chain_debug = {\n",
    "    \"context\": retriever,  # 컨텍스트를 가져오는 retriever\n",
    "    \"question\": DebugPassThrough()  # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
    "} | DebugPassThrough() | ContextToText() | contextual_prompt | model\n",
    "\n",
    "# 질문 실행 및 각 단계 출력 확인\n",
    "response = rag_chain_debug.invoke(\"강사 이름은?\")\n",
    "print(\"Final Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAISS 인덱스의 저장 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 저장\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "# 저장된 인덱스 로드\n",
    "new_vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAISS 데이터베이스 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = FAISS.from_texts([\"문서 1 내용\"], embeddings)\n",
    "db2 = FAISS.from_texts([\"문서 2 내용\"], embeddings)\n",
    "\n",
    "# 병합\n",
    "db1.merge_from(db2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ch3_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
